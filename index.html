<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
  body {
    font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
    font-weight:300;
    font-size:18px;
    margin-left: auto;
    margin-right: auto;
    width: 1100px;
  }
  
  h1 {
    font-size:32px;
    font-weight:300;
  }
  
  .disclaimerbox {
    background-color: #eee;   
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }
  
  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }
  
  img.rounded {
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }
  
  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }
  
  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }
  
  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
    0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
    5px 5px 0 0px #fff, /* The second layer */
    5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
    10px 10px 0 0px #fff, /* The third layer */
    10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
    15px 15px 0 0px #fff, /* The fourth layer */
    15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
    20px 20px 0 0px #fff, /* The fifth layer */
    20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
    25px 25px 0 0px #fff, /* The fifth layer */
    25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }

  .paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
    0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
    0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
    5px 5px 0 0px #fff, /* The second layer */
    5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
    10px 10px 0 0px #fff, /* The third layer */
    10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }
  
  .vert-cent {
    position: relative;
    top: 50%;
    transform: translateY(-50%);
  }
  
  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
</style>

<html>
<head>
  <title>Trust, but Verify</title>
  <meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
  <meta property="og:title" content="Creative and Descriptive Paper Title." />
  <meta property="og:description" content="Paper description." />

  <!-- Get from Google Analytics -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src=""></script> 
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-75863369-6');
  </script>
</head>

<body>
  <br>
  <center>
    <span style="font-size:40px">Trust, but Verify (TbV) <br> </span>
     <span style="font-size:30px">Cross-Modality Fusion for HD Map Change Detection</span>
    <span style="font-size:20px"><br>NeurIPS 2021</span>
    <table align=center width=600px>
      <table align=center width=600px>
        <tr>

          <td align=center width=100px>
            <center>
              <span style="font-size:20px"><a href="https://johnwlambert.github.io/">John Lambert</a></span>
            </center>
          </td>

          <td align=center width=100px>
            <center>
              <span style="font-size:20px"><a href="https://faculty.cc.gatech.edu/~hays/">James Hays</a></span>
            </center>
          </td>

        </tr>
      </table>
      <table align=center width=250px>
        <tr>
          <td align=center width=120px>
            <center>
              <span style="font-size:24px"><a href='https://arxiv.org/abs/2212.07312'>[Paper]</a></span>
            </center>
          </td>
          <td align=center width=120px>
            <center>
              <span style="font-size:24px"><a href='https://github.com/johnwlambert/tbv'>[GitHub]</a></span><br>
            </center>
          </td>
        </tr>
      </table>
    </table>
  </center>

  <center>
    <table align=center width=850px>
      <tr>
        <td width=260px>
          <center>
            <img class="round" style="width:660px" src="https://user-images.githubusercontent.com/16724970/197935230-a504bc5e-9671-4b28-886c-a1819965b489.gif"/>
          </center>
        </td>
      </tr>
    </table>

<!--     <table align=center width=850px>
      <tr>
        <td>
          
        </td>
      </tr>
    </table> -->
  </center>

  <hr>

  <table align=center width=850px>
    <center><h1>Abstract</h1></center>
    <tr>
      <td>
        High-definition (HD) map change detection is the task of determining when sensor data and map data are no longer in agreement with one another due to real-world changes. We collect the first dataset for the task, which we entitle the Trust, but Verify (TbV) dataset, by mining thousands of hours of data from over 9 months of autonomous vehicle fleet operations. We present learning-based formulations for solving the problem in the birdâ€™s eye view and ego-view. Because real map changes are infrequent and vector maps are easy to synthetically manipulate, we lean on simulated data to train our model. Perhaps surprisingly, we show that such models can generalize to real world distributions. The dataset, consisting of maps and logs collected in six North American cities, is one of the largest AV datasets to date with more than 7.9 million images. We make the data available to the public, along with code and models under the the CC BY-NC-SA 4.0 license.
      </td>
    </tr>
  </table>
  <br>



  <hr>




Dealing with real world changes and stale maps is a constant problem for large-scale self-driving efforts today. And this is the first public dataset for it. Here's an example of a stale map, after a bike lane was added:


  <center>
    <table align=center width=850px>
      <tr>
        <td width=260px>
          <center>
            <img class="round" style="width:660px" src="https://user-images.githubusercontent.com/16724970/197935420-ffc50358-5069-4256-822f-86a75c7d921f.gif"/>
          </center>
        </td>
      </tr>
    </table>
    
    <table align=center width=850px>
      <tr>
        <td>
          
        </td>
      </tr>
    </table>
  </center>


HD maps have proven to be an effective way to assist self-driving vehicles in **safely** navigating difficult intersections and city streets. Unprotected lefts are not easy!


  <center>
    <table align=center width=850px>
      <tr>
        <td width=260px>
          <center>
            <img class="round" style="width:660px" src="./assets/RJe_lgB_hstack.gif"/>
          </center>
        </td>
      </tr>
    </table>
    
    <table align=center width=850px>
      <tr>
        <td>
          
        </td>
      </tr>
    </table>
  </center>

But we live in a constantly-changing world, so maps need to constantly be updated. Training and evaluating models in academia for this has been difficult without public datasets for the task.

  <center>
    <table align=center width=850px>
      <tr>
        <td width=260px>
          <center>
            <img class="round" style="width:660px" src="https://user-images.githubusercontent.com/16724970/197935637-adb17b60-90ef-4b2e-856f-9a1667242f76.gif"/>
          </center>
        </td>
      </tr>
    </table>
    
    <table align=center width=850px>
      <tr>
        <td>
          
        </td>
      </tr>
    </table>
  </center>

We lean on using real sensor data with synthetically modified maps to train new models, and evaluate on over 200 logs with real world map changes. We mined large-scale fleet data for almost a year to collect interesting map changes. Here's an example from downtown Pittsburgh:

  <center>
    <table align=center width=850px>
      <tr>
        <td width=260px>
          <center>
            <img class="round" style="width:660px" src="https://user-images.githubusercontent.com/16724970/197935690-fdddf2e5-0218-4762-8db3-6e117de9596d.gif"/>
          </center>
        </td>
      </tr>
    </table>
    
    <table align=center width=850px>
      <tr>
        <td>
          
        </td>
      </tr>
    </table>
  </center>

Each log is about 54 seconds long, and there are over 1000 of them in total, across train/val/test. This amounts to about 16 continuous hours of sensor data -- 8 Million images and 1 TB of data. An example from Stanford campus:

  <center>
    <table align=center width=850px>
      <tr>
        <td width=260px>
          <center>
            <img class="round" style="width:660px" src="https://user-images.githubusercontent.com/16724970/197935757-92aa8920-0c40-4d1e-8abf-49786e2ada4b.gif"/>
          </center>
        </td>
      </tr>
    </table>
    
    <table align=center width=850px>
      <tr>
        <td>
          
        </td>
      </tr>
    </table>
  </center>

Every log comes with a paired HD map, with lane boundaries & markings in 3d, along with a ground height map, drivable area polygons, and annotated crosswalks.

By number of driving hours, this dataset is about 3x the size of nuScenes (15.5 hours vs. 5.5 hours), and comes with annotated HD maps for all the logs. It's captured in 6 cities -- Austin, Detroit, Miami, Palo Alto, Pittsburgh, and Washington DC -- across 4 seasons.

  <center>
    <table align=center width=850px>
      <tr>
        <td width=260px>
          <center>
            <img class="round" style="width:660px" src="./assets/Rvsu_rdWm_hstacked.gif"/>
          </center>
        </td>
      </tr>
    </table>
    
    <table align=center width=850px>
      <tr>
        <td>
          
        </td>
      </tr>
    </table>
  </center>


  <br>
  <hr>
  
  <center><h1>Dataset Download</h1></center>

  <table align=center width=800px>
    <br>
    <tr><center>
      <span style="font-size:28px">&nbsp;<a href='https://www.argoverse.org/av2.html#mapchange-link'>[Download link]</a>
      </center>
    </span>
  </table>

  <br>
  <hr>


  <center><h1>Code</h1></center>

  <table align=center width=420px>
    <center>
      <tr>
        <td>
        </td>
      </tr>
    </center>
  </table>

  <table align=center >
    <tr>
      <td align=center >
        <center>
             <td><a href='https://github.com/johnwlambert/tbv'><img src="https://user-images.githubusercontent.com/16724970/207622129-caa6ccab-ebb3-483b-a713-3094a071fa54.png"  height="100"></a></td>
        </center>
      </td>
    </tr>
  </table>

  <table align=center width=400px>
    <tr>
      <td align=center width=400px>
        <center>
             <td><img src="https://user-images.githubusercontent.com/16724970/165433493-219d9070-c157-4b05-9862-70fe5b6eec3d.jpg" height="200"></td>
             <td><img src="https://user-images.githubusercontent.com/16724970/165433531-26994c65-62e2-4ba4-b2ce-6c7669d0d864.png" height="200"></td>
             <td><img src="https://user-images.githubusercontent.com/16724970/165433535-84be378b-620e-4523-97f6-01cb37536c1d.jpg" height="200"></td>
             <td><img src="https://user-images.githubusercontent.com/16724970/165433547-9479430c-432c-4722-bdfa-acec0c3dab44.png" height="200"></td>
        </center>
      </td>
    </tr>
  </table>

  <table align=center width=850px>
    <center>
      <tr>
        <td>
          We make our training code, model inference code, pretrained models, and rendering code publicly available on GitHub.
        </td>
      </tr>
    </center>
  </table>
  <table align=center width=800px>
    <br>
    <tr><center>
      <span style="font-size:28px">&nbsp;<a href='https://github.com/johnwlambert/tbv'>[GitHub]</a>
      </center>
    </span>
  </table>
  <br>
  <hr>
  <table align=center width=450px>
    <center><h1>Paper and Supplementary Material</h1></center>
    <tr>
      <td><a href=""><img class="layered-paper-big" style="height:175px" src="./assets/tbv-paper-thumbnail.png"/></a></td>
      <td><span style="font-size:14pt">John Lambert and James Hays.<br>
        <b>Trust, but Verify: Cross-Modality Fusion for HD Map Change Detection.</b><br>
        In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks (NeurIPS), 2021.<br>
        (hosted on <a href="https://arxiv.org/pdf/2212.07312">ArXiv</a>)<br>
        <!-- (<a href="https://arxiv.org/pdf/2212.07312">camera ready</a>)<br> -->
        <span style="font-size:4pt"><a href="https://arxiv.org/pdf/2212.07312"><br></a>
        </span>
      </td>
    </tr>
  </table>
  <br>

  <table align=center width=600px>
    <tr>
      <td><span style="font-size:14pt"><center>
        <a href="./assets/bibtex/lambert21neurips_tbv.txt">[Bibtex]</a>
      </center></td>
    </tr>
  </table>

  <hr>
  <br>


  <center><h1>Talk</h1></center>
  Invited talk at the CVPR 2021 VOCVALC Workshop:
  <p align="center">
    <iframe width="660" height="395" src="https://www.youtube.com/embed/JeTZbCuyeM8?t=3735" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen align="center"></iframe>
  </p>

  NeurIPS 2021 SlidesLive: <span ><a href='https://neurips.cc/virtual/2021/poster/29833'>Talk</a></span>

  <table align=center width=800px>
    <br>
    <tr>
      <center>
        <span style="font-size:28px"><a href='https://neurips.cc/virtual/2021/poster/29833'>[Slides]</a>
        </span>
      </center>
    </tr>
  </table>
  <hr>



  <table align=center width=900px>
    <tr>
      <td width=400px>
        <left>
          <center><h1>Acknowledgements</h1></center>
          Thanks to Phillip Isola and Richard Zhang the <a href="https://github.com/richzhang/webpage-template">page template</a>.
        </left>
      </td>
    </tr>
  </table>

<br>
</body>
</html>

